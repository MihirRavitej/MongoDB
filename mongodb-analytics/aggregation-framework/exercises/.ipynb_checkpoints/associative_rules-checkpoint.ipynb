{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies Installation\n",
    "Before we get started, let's make sure we have all dependencies installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip3 install pymongo dateparser sklearn pandas numpy pprint scipy matplotlib seaborn mlxtend\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Association Rules\n",
    "\n",
    "In this lab you'll be tasked with finding association rules. You'll have to implement some functions to perform one-hot encoding (more below), as well as find the answer to the following question.\n",
    "\n",
    "> What is the support, confidence, and lift value for the association (Class_First, Gender_Female) -> (Survived)\n",
    "\n",
    "Provide your answer as the sum of support, confidence, and lift rounded to the nearest 100th:\n",
    "\n",
    "    support=5.98134, confidence=0.01738, lift=0.57823\n",
    "    5.98134 + 0.01738 + 0.57823 = 6.57695\n",
    "    which rounds to 6.58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import dateparser\n",
    "import pymongo\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pymongo Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pymongo driver configuration\n",
    "course_cluster_uri = \"mongodb://agg-student:agg-password@cluster0-shard-00-00-jxeqq.mongodb.net:27017,cluster0-shard-00-01-jxeqq.mongodb.net:27017,cluster0-shard-00-02-jxeqq.mongodb.net:27017/test?ssl=true&replicaSet=Cluster0-shard-0&authSource=admin\"\n",
    "course_client = pymongo.MongoClient(course_cluster_uri)\n",
    "titanic = course_client['coursera-agg']['titanic']\n",
    "titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting our data from MongoDB\n",
    "\n",
    "Associative learning is most simply done with one-hot encoding. You'll need to transform the most relevant points of data, which we've been exploring. Again, most of this is done for you.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keys\n",
    "\n",
    "To get all of the keys possible, we'll use a `$group` stage to collect all keys encountered. We'll do this by using the `$map` expression, converting the object to an array and using that as the input, and then extracting only the key value.\n",
    "\n",
    "We are also gathering the `_id` of documents encountered so we can perform a later `$lookup`. We do this rather than pushing the entire document into an array to keep memory and/or disk impact as low as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_analysis = {\n",
    "    \"$group\": {\n",
    "            \"_id\":  0,\n",
    "            \"all_keys\": {\n",
    "                \"$addToSet\": {\n",
    "                    \"$map\": {\n",
    "                        \"input\": {\"$objectToArray\": \"$$CURRENT\"},\n",
    "                        \"in\": \"$$this.k\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"source_ids\": {\n",
    "                \"$push\": \"$$CURRENT._id\"\n",
    "            }\n",
    "        }\n",
    "}\n",
    "pprint.pprint(key_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "We now have an array of keys that is an array of arrays that are all keys encountered on documents. We now have to clean this up to create a flat array that has no duplicate values. This will result in an array of every key encountered, no less and no more.\n",
    "\n",
    "To accomplish this, we use the `$reduce` expression, specifying the `all_keys` array we just created as the input. We give it an initial value of an empty array, and then use the `$setUnion` operator to ensure we don't have any duplicates. This will result in a flat array of unique keys, exactly what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_keys = {\n",
    "    \"$addFields\": {\n",
    "        \"all_keys\": {\n",
    "            \"$reduce\": {\n",
    "                \"input\": \"$all_keys\",\n",
    "                \"initialValue\": [],\n",
    "                \"in\": {\n",
    "                    \"$setUnion\": [\"$$value\", \"$$this.k\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "pprint.pprint(clean_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unwinding `source_ids`\n",
    "We now unwind the `ObjectId`s in `source_ids` to perform our lookup. This creates a new document for every element that was in `$source_ids`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_unwind = {\n",
    "    \"$unwind\": \"$source_ids\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup and Second Unwind\n",
    "\n",
    "We then use `$lookup` to build up our documents again. We're guaranteed only one document per lookup because we are using `_id`, which has a unique constraint in the database. We immediately follow with an `$unwind` stage.\n",
    "\n",
    "Internally, these are coalesced so really are one stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup = {\n",
    "    \"$lookup\": {\n",
    "        \"from\": \"titanic\",\n",
    "        \"localField\": \"source_ids\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"source_docs\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "second_unwind = {\n",
    "    \"$unwind\": \"$source_docs\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in Missing Values\n",
    "\n",
    "We have to fill in missing values from the current document. To do this, we iterate over the difference between `all_keys` that we calculated previously, and an object called `$$curent_obj` that will be provided for us.\n",
    "\n",
    "We set the initial value to `current_obj`, and then append the missing keys with default values of `\"\"`, an empty string. We do this in order to avoid conflicts with our one-hot encoding functions which are further in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fill_missing_with_blank = {\n",
    "    \"$reduce\": {\n",
    "        \"input\": {\n",
    "            \"$setDifference\": [\"$all_keys\", \"$$current_obj\"]\n",
    "        },\n",
    "        \"initialValue\": \"$$current_obj\",\n",
    "        \"in\": {\"$concatArrays\": [\"$$value\", [{\"k\": \"$$this\", \"v\": \"\"}]]}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "We need to convert the relevant data points to one-hot encoded. Binary data, like our survived field, is already encoded this way, where 1 is yes and 0 is no.\n",
    "\n",
    "However, information like class and gender are not. We need to convert information like this to something like `Gender_Female` and `Gender_Male`. This is because the algorithms used for associative rule learning interpret 1 and **yes** and 0 as **no** in order to form correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_class():\n",
    "    c_map = {\n",
    "        1: \"Class_First\",\n",
    "        2: \"Class_Second\",\n",
    "        3: \"Class_Third\",\n",
    "        \"\": \"Class_Unknown\"\n",
    "    }\n",
    "\n",
    "    def convert_class(c):\n",
    "        c_l = map(\n",
    "            lambda x: {\n",
    "                \"k\": f\"{c_map.get(x)}\", \"v\": 1 if c == x else 0\n",
    "            },\n",
    "            c_map.keys()\n",
    "        )\n",
    "        return [doc for doc in c_l]\n",
    "    c_cases = map(\n",
    "        lambda x: {\n",
    "            \"case\": {\n",
    "                \"$eq\": [\"$$this.v\", x]\n",
    "            },\n",
    "            \"then\": {\n",
    "                \"$concatArrays\": [\n",
    "                    \"$$value\",\n",
    "                    convert_class(x)\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        c_map.keys()\n",
    "    )\n",
    "    return {\n",
    "        \"$switch\": {\"branches\": [case for case in c_cases] }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Gender\n",
    "\n",
    "\n",
    "We know gender was a huge factor in the survival rate. You will need to implement the one-hot encoding transformation for gender below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo\n",
    "#def one_hot_gender():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Point of Embarkation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_embarkation():\n",
    "    p_map = {\n",
    "        \"C\": \"C\",\n",
    "        \"S\": \"S\",\n",
    "        \"Q\": \"Q\",\n",
    "        \"\": \"?\"\n",
    "    }\n",
    "\n",
    "    def convert_location(point):\n",
    "        o_l = map(\n",
    "            lambda x: {\n",
    "                \"k\": f\"embarked_{p_map.get(x)}\", \"v\": 1 if point == x else 0\n",
    "            },\n",
    "            p_map.keys()\n",
    "        )\n",
    "        return [doc for doc in o_l]\n",
    "\n",
    "    point_cases = map(\n",
    "        lambda x: {\n",
    "            \"case\": {\n",
    "                \"$eq\": [\"$$this.v\", x]\n",
    "            },\n",
    "            \"then\": {\n",
    "                \"$concatArrays\": [\n",
    "                    \"$$value\",\n",
    "                    convert_location(x)\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        p_map.keys()\n",
    "    )\n",
    "    return { \"$switch\": {\"branches\": [case for case in point_cases] } }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_age():\n",
    "    a_map = {\n",
    "        5: \"5\",\n",
    "        10: \"10\",\n",
    "        20: \"20\",\n",
    "        40: \"40\",\n",
    "        80: \"80\",\n",
    "        160: \"160\"\n",
    "    }\n",
    "\n",
    "    def convert_age(age):\n",
    "        if age == \"\":\n",
    "            age = 5\n",
    "        a_l = map(\n",
    "            lambda x: {\n",
    "                \"k\": f\"age_under_{a_map.get(x)}\", \"v\": 1 if age == x else 0\n",
    "            },\n",
    "            a_map.keys()\n",
    "        )\n",
    "        return [doc for doc in a_l]\n",
    "\n",
    "    age_cases = map(\n",
    "        lambda x: {\n",
    "            \"case\": {\n",
    "                \"$lte\": [\"$$this.v\", x]\n",
    "            },\n",
    "            \"then\": {\n",
    "                \"$concatArrays\": [\n",
    "                    \"$$value\",\n",
    "                    convert_age(x)\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        a_map.keys()\n",
    "    )\n",
    "    \n",
    "    age_cases = [case for case in age_cases]\n",
    "    age_cases.append({\n",
    "        \"case\": {\n",
    "                \"$eq\": [\"$$this.v\", \"\"]\n",
    "            },\n",
    "            \"then\": {\n",
    "                \"$concatArrays\": [\n",
    "                    \"$$value\",\n",
    "                    convert_age(5)\n",
    "                ]\n",
    "            }\n",
    "    })\n",
    "\n",
    "    return { \"$switch\": {\"branches\": [case for case in age_cases] } }    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Case Helper Function\n",
    "\n",
    "We'll use another case helper function for the key names. Pay attention to the `#todo` below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo - add gender and a call to one_hot_gender in the key_map dictionary below\n",
    "def key_case(which_key):\n",
    "    key_map = {\n",
    "        \"point_of_embarkation\": one_hot_embarkation,\n",
    "        \"class\": one_hot_class,\n",
    "        \"age\": one_hot_age\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"case\": {\n",
    "            \"$eq\": [\"$$this.k\", f\"{which_key}\"]\n",
    "        },\n",
    "        \"then\": key_map[which_key]()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the encoding\n",
    "\n",
    "Here is where the one-hot encoding will be performed. After ensuring all missing values are filled with blanks, we use our `key_case` helper function to get the encoding for the specific keys. Pay attention to the `#todo` below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remember to include a call to key_case(\"gender\") after you implement it!\n",
    "encoding_utility = {\n",
    "    \"$reduce\": {\n",
    "        \"input\": fill_missing_with_blank,\n",
    "        \"initialValue\": [],\n",
    "        \"in\": {\n",
    "            \"$switch\": {\n",
    "                \"branches\": [\n",
    "                    key_case(\"point_of_embarkation\"),\n",
    "                    key_case(\"class\"),\n",
    "                    key_case(\"age\")\n",
    "                ],\n",
    "                \"default\": {\n",
    "                    \"$concatArrays\": [\"$$value\", [\"$$this\"]]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Stage\n",
    "\n",
    "This is the stage that will use the `convert_non_numerics` variable we just defined.\n",
    "\n",
    "The `$replaceRoot` stage will replace the current document with the results of the expression provided to `newRoot`, which is using `$arrayToObject`, converting the array of key/value pairs back to an object.\n",
    "\n",
    "Notice the use of the `$let` expression here, allowing us to define a variable for use within its scope. This is where the `current_obj` value is being created used in `fill_missing_with_0` and `convert_non_numerics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding_stage = {\n",
    "    \"$replaceRoot\": {\n",
    "        \"newRoot\": {\n",
    "            \"$arrayToObject\": {\n",
    "                \"$let\": {\n",
    "                    \"vars\": {\n",
    "                        \"current_obj\": {\"$objectToArray\": \"$source_docs\"}\n",
    "                    },\n",
    "                    \"in\": encoding_utility\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Fields\n",
    "\n",
    "Lastly, we have the final `$project` stage that will remove fields returned to us. Remember to remove age from this stage after completing the one-hot encoding function for it above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remember to remove gender from this project after you've implement the one-hot encoding for it above\n",
    "redacting_project = {\n",
    "    \"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"name\": 0,\n",
    "        \"ticket_number\": 0,\n",
    "        \"cabin\": 0,\n",
    "        \"gender\": 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the Pipeline\n",
    "\n",
    "We now construct the pipeline from our variables that represent stages: `redacting_project`, `data_cleaning`, `lookup_and_unwind`, `first_unwind`, `get_all_keys`, `key_analysis_and_set_building`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    key_analysis,\n",
    "    clean_keys,\n",
    "    first_unwind,\n",
    "    lookup,\n",
    "    second_unwind,\n",
    "    encoding_stage,\n",
    "    redacting_project\n",
    "]\n",
    "print(json.dumps(pipeline, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the pandas Dataframe from MongoDB\n",
    "\n",
    "Here you will need to construct the DataFrame. Assign it to the variabled `df` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(list(titanic.aggregate(pipeline)))\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Is one-hot encoding better for KMeans?\n",
    "\n",
    "Let's see if the results of k-means clustering are better on this new data format than simply \"hashing\" non-numerics and getting a value, i.e. 1 for female and 0 for male and (1, 2, 3) to represent their class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(['survived'], axis=1)).astype('float64')\n",
    "X = preprocessing.scale(X)\n",
    "y = np.array(df[['survived']])\n",
    "clf = KMeans(n_clusters=2, n_init=20)\n",
    "clf.fit(X)\n",
    "correct = 0\n",
    "for i in range(len(X)):\n",
    "    predict_me = np.array(X[i].astype(float))\n",
    "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "    prediction = clf.predict(predict_me)\n",
    "    if prediction[0] == y[i]:\n",
    "        correct += 1\n",
    "accuracy = correct/len(X)\n",
    "if accuracy < .50:\n",
    "    accuracy = 1 - accuracy\n",
    "\n",
    "print(\"Correct survival grouping: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gratuitous Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(x=\"Gender_Female\", y=\"Class_First\", hue=\"survived\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori\n",
    "First, we'll use the `apriori` algorithm from `mlxtend` to extract frequent itemsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assocs = apriori(df, min_support=0.09, use_colnames=True)\n",
    "assocs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules\n",
    "\n",
    "Now we form the association rules. Try adjusting the `min_threshold` along with the `metric` to find interesting associations. For example, which class appears to be highly associated with `parents_children`? Go back and add a one-hot encoding function for `parents_children` and see if the results are more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(assocs, metric=\"lift\", min_threshold=.1)\n",
    "rules.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
